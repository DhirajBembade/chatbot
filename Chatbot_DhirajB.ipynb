{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmI17lfSIwyc"
      },
      "outputs": [],
      "source": [
        "!pip install -qq langchain wget llama-index cohere llama-cpp-python\n",
        "\n",
        "# lanchain\n",
        "# wget - downlode the model from hugging face\n",
        "# llama_index - create llm object\n",
        "# cohrere - dependency required by llamaindex\n",
        "# connect to high  runtime type T4GPU or A100\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "\n",
        "def bar_custom(current, total, width=80):\n",
        "    print(\"Downloading %d%% [%d / %d] bytes\" % (current / total * 100, current, total))\n",
        "\n",
        "model_url = \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q2_K.gguf\"\n",
        "wget.download(model_url, bar=bar_custom)\n",
        "\n",
        "#https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF"
      ],
      "metadata": {
        "id": "YoJ3xVa4-oxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#install streamlit -API UI\n",
        "\n",
        "!pip -q install streamlit"
      ],
      "metadata": {
        "id": "jWN0wHWa_eNB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, ServiceContext, VectorStoreIndex"
      ],
      "metadata": {
        "id": "J3bLybUJJNcF"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install llama-index-embeddings-huggingface\n",
        "%pip install llama-index-llms-llama-cpp"
      ],
      "metadata": {
        "id": "41YIWCwhL_mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.llama_cpp.llama_utils import (\n",
        "    messages_to_prompt,\n",
        "    completion_to_prompt,\n",
        ")"
      ],
      "metadata": {
        "id": "7qph8cGrMFVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## lanchain schema -imporing HumanMessage , lanchain takes the context I/P\n",
        "# in model path - provide your model path link\n",
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from llama_index.core import (\n",
        "  SimpleDirectoryReader,\n",
        "  VectorStoreIndex,\n",
        "  ServiceContext,\n",
        ")\n",
        "\n",
        "from llama_index.llms.llama_cpp import LlamaCPP\n",
        "from llama_index.llms.llama_cpp.llama_utils import (\n",
        "  messages_to_prompt,\n",
        "  completion_to_prompt,\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.schema import(SystemMessage, HumanMessage, AIMessage)\n",
        "\n",
        "\n",
        "def init_page() -> None:\n",
        "  st.set_page_config(\n",
        "    page_title=\"Personal Chatbot\"\n",
        "  )\n",
        "  st.header(\"Support Chatbot\")\n",
        "  st.sidebar.title(\"Options\")\n",
        "\n",
        "def select_llm() -> LlamaCPP:\n",
        "  return LlamaCPP(\n",
        "    model_path=\"/content/llama-2-7b-chat.Q2_K.gguf\",\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=500,\n",
        "    context_window=3900,\n",
        "    generate_kwargs={},\n",
        "    model_kwargs={\"n_gpu_layers\":1},\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        "  )\n",
        "\n",
        "def init_messages() -> None:\n",
        "  clear_button = st.sidebar.button(\"Clear Conversation\", key=\"clear\")\n",
        "  if clear_button or \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = [\n",
        "      SystemMessage(\n",
        "        content=\"you are a helpful AI assistant. Reply your answer in markdown format.\"\n",
        "      )\n",
        "    ]\n",
        "\n",
        "def get_answer(llm, messages) -> str:\n",
        "  response = llm.complete(messages)\n",
        "  return response.text\n",
        "\n",
        "def main() -> None:\n",
        "  init_page()\n",
        "  llm = select_llm()\n",
        "  init_messages()\n",
        "\n",
        "  if user_input := st.chat_input(\"Input your question!\"):\n",
        "    st.session_state.messages.append(HumanMessage(content=user_input))\n",
        "    with st.spinner(\"Bot is typing ...\"):\n",
        "      answer = get_answer(llm, user_input)\n",
        "      print(answer)\n",
        "    st.session_state.messages.append(AIMessage(content=answer))\n",
        "\n",
        "\n",
        "  messages = st.session_state.get(\"messages\", [])\n",
        "  for message in messages:\n",
        "    if isinstance(message, AIMessage):\n",
        "      with st.chat_message(\"assistant\"):\n",
        "        st.markdown(message.content)\n",
        "    elif isinstance(message, HumanMessage):\n",
        "      with st.chat_message(\"user\"):\n",
        "        st.markdown(message.content)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "id": "UT0LZ-u9_eIZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6acac2e4-d35a-42cf-9457-82011fcdb2d6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "pEg1EOsr_eFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"#step -\n",
        "1] click on your url - link\n",
        "2] copy external url and paste in tunnel password\n",
        "3] submit end ask question to chatbot, happy questioning and recommendations\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "xmzY0ITY_d-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Updated Chatbot\n",
        "\n",
        "Once the Streamlit app is running, you can interact with the updated chatbot through the web interface. Simply enter your query in the input box, and the chatbot will respond with a helpful answer.\n",
        "\n",
        "The updated chatbot is designed to assist with a variety of queries, providing accurate and relevant information based on the input provided."
      ],
      "metadata": {
        "id": "updated_chatbot_section"
      }
    }
  ]
}
